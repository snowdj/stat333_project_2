import numpy as np
import pandas as pd
import pickle
import h5py
import os
from math import floor
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential, model_from_json
from keras.layers import Embedding, Dense, Flatten, Conv1D
from keras.layers import MaxPooling1D
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import RMSprop


TRAIN_TEXT = '../../data/text_train.csv'
TEST_TEXT = '../../data/text_test.csv'
TUNE_TEXT = '../../data/text_validate.csv'
MINI_TEXT = '../../data/text_mini.csv'
EMBEDDING = '../../static/glove100d.pickle'
MODEL_SAVE = './config/my_model.h5'
WEIGHT_SAVE = './config/my_weights.h5'
MODEL_SAVE_JSON = './config/my_model.txt'
GLOVE_SUBSET = './config/glove_subset.pickle'
PREDICTION_CSV = './config/prediction.csv'
# Only work with top 10000 common words
# TOP_WORDS_NUM = 10000
# We have to get a unified max length, otherwise we cannot predict other set
MAX_LENGTH = 300
# Proportion of data used for validation
VALIDATION_PERCENT = 0.01
# The dimension of word vector
WORD_VECTOR_SIZE = 100
KERNEL_SIZE = 5
POOL_SIZE = 5
FILTER = 128
FC_SIZE = 400
CLASS_NUM = 5
EPOCH = 2
BATCH_SIZE = 128
ETA = 0.0001
DEBUG = False
PREDICT = True


def get_text_label(input_csv):
    """ Make a list of texts, and a np array of labels (rates). Directly use
        `../subset/text_*.csv` files generated by `../subset/parser.py`.
    """
    table = pd.read_csv(input_csv, header=0)
    # Use native list for text in order to integrate with keras api
    text = table['text'].tolist()
    label = table['stars'].values if 'stars' in table.columns else None

    return text, label


def tensorize_words(train_text, all_text, label):
    """ Make tensors from text features. We will use a super helpful tool
        `keras.preprocessing.text.Tokenizer` to work with the words.

        Arguments:
            train_text, the text will be used to train.
            all_text, the text to be tensorized, we want to build a
                vocab bank for all the text will be used.
            label, the label for the train_text.

        Returns:
            Training features, training labels.
            Tuning features, tuning labels.
            tokenizer: a fitted tokenizer, for future encoding uses.

        This function should be only be used in one case now:
        1. tensorize_words(train_text, all_text, lable), when it is the first
            time to fit tokenizer.

        ** If you want to tensorize words for prediction, please directly use
           ` tokenizer.texts_to_sequences()` and `pad_sequences()` functions.
    """
    # Convert text to a sequence, use common word counting index (rank)
    # No top common word restriction
    tokenizer = Tokenizer()
    # Count in each element (initializing)
    tokenizer.fit_on_texts(all_text)

    # Count in the document
    sequence = tokenizer.texts_to_sequences(train_text)
    data = pad_sequences(sequence, maxlen=MAX_LENGTH)

    # Use one-hot to encode ratings, `to_categorical`
    # would use 0-index so we subtract 1 here
    label = to_categorical(label - 1)

    # Shuffle the features and labels systematically
    index = np.arange(data.shape[0])
    np.random.shuffle(index)
    data = data[index]
    label = label[index]

    # Divide train and tuning set
    validate_num = floor(data.shape[0] * VALIDATION_PERCENT)
    x_tune = data[:validate_num]
    y_tune = label[:validate_num]
    x_train = data[validate_num:]
    y_train = label[validate_num:]

    return x_train, y_train, x_tune, y_tune, tokenizer


def embedding_words(word_index):
    """ Make a embedding layer using the GloVe pre-trained word embedding
        dictionary.
    """
    # Fill out the `embedded` first, since dictionary has no order
    # We should +1 here, because the word_index uses 1-index for word rank!
    embedded = np.empty([len(word_index) + 1, WORD_VECTOR_SIZE],
                        dtype='float32')

    # Get the embedding dictionary, if there is no subset yet, we make one
    if not os.path.exists(GLOVE_SUBSET):
        make_glove_subset()

    with open(GLOVE_SUBSET, 'rb') as fp:
        embedding_dict = pickle.load(fp)

    # Embed the words (we want to embed all the words here, to be consistent
    # with the validation and test set vocabulary)

    for key, value in word_index.items():
        # We use the word rank from word_index as key in embedded
        if (key in embedding_dict):
            embedded[value] = embedding_dict[key]

    return embedded


def train(x_train, y_train, x_tune, y_tune, embedded):
    """ Use a convolutional network, with embedded layer as the first layer
        to train a deep learning model.
    """
    # Compute the max sequence length
    # max_seq_length = max(x_train.shape[1], x_tune.shape[1])
    # Now use global variable MAX_SEQUENCE_LENGTH

    # Build the model
    leaky = 0.01
    model = Sequential()

    # Embedding layer
    model.add(Embedding(input_shape=(MAX_LENGTH,),
                        input_dim=embedded.shape[0],
                        output_dim=WORD_VECTOR_SIZE,
                        embeddings_regularizer=None,
                        weights=[embedded],
                        trainable=False))

    # We use a CP-CP-CP model
    model.add(Conv1D(filters=FILTER, kernel_size=KERNEL_SIZE))
    model.add(LeakyReLU(alpha=leaky))
    model.add(MaxPooling1D(pool_size=POOL_SIZE))
    model.add(Conv1D(filters=FILTER, kernel_size=KERNEL_SIZE))
    model.add(LeakyReLU(alpha=leaky))
    model.add(MaxPooling1D(pool_size=POOL_SIZE))
    model.add(Conv1D(filters=FILTER, kernel_size=KERNEL_SIZE))
    model.add(LeakyReLU(alpha=leaky))
    model.add(MaxPooling1D(pool_size=POOL_SIZE))

    # Tradition to add a FC layer
    model.add(Flatten())
    model.add(Dense(units=FC_SIZE))
    model.add(LeakyReLU(alpha=leaky))

    # Output layer
    model.add(Dense(units=CLASS_NUM, activation="softmax"))

    # Set up optimizer
    rmsprop = RMSprop(lr=ETA)
    model.compile(loss='categorical_crossentropy',
                  optimizer=rmsprop,
                  metrics=['acc'])

    # Start training
    model.fit(x_train, y_train,
              validation_data=(x_tune, y_tune),
              epochs=EPOCH,
              batch_size=BATCH_SIZE,
              shuffle=True)

    # Save the model and weights(the following two functions do not work)
    # model.save(MODEL_SAVE)
    # model.save_weights(WEIGHT_SAVE)

    # Only save weight if not debugging
    save_model(model)
    if not DEBUG:
        save_weight(model)


def save_weight(model):
    """ Have trouble with saving the model and weights, manually save the
        weights, based on the (issue)[https://github.com/farizrahman4u/seq2seq
        /issues/129].
    """
    file = h5py.File(WEIGHT_SAVE, 'w')
    weight = model.get_weights()
    for i in range(len(weight)):
        file.create_dataset('weight' + str(i), data=weight[i])
    file.close()


def load_weight(model):
    """ Have trouble with saving the model and weights, manually load the
        weights, based on the (issue)[https://github.com/farizrahman4u/seq2seq
        /issues/129].
    """
    file = h5py.File(WEIGHT_SAVE, 'r')
    weight = []
    for i in range(len(file.keys())):
        weight.append(file['weight' + str(i)][:])
    model.set_weights(weight)


def save_model(model):
    """ Have trouble with saving the model and weights, use json to store
        the model.
    """
    json_string = model.to_json()
    with open(MODEL_SAVE_JSON, 'w') as fp:
        fp.write(json_string)


def load_model():
    """ Have trouble with saving the model and weights, use json to load
        the model.
    """
    with open(MODEL_SAVE_JSON, 'r') as fp:
        json_string = fp.read()
        model = model_from_json(json_string)
        return model


def make_glove_subset():
    """ GloVe is too large, we just make a subset dictionary for all the given
        text (in train, validation, and test subset)
    """
    # Join all the text
    text = []
    for file_name in [TRAIN_TEXT, TUNE_TEXT, TEST_TEXT]:
        table = pd.read_csv(file_name, header=0)
        text += table['text'].tolist()

    tokenizer = Tokenizer()
    # Count in each element (initializing)
    tokenizer.fit_on_texts(text)

    # Load the big dictionary
    with open(EMBEDDING, 'rb') as fp:
        embedding_dict = pickle.load(fp)

    # Make a subset dictionary
    subset = {}
    for key, value in tokenizer.word_index.items():
        # We use the word rank from word_index as key in embedded
        if (key in embedding_dict):
            subset[key] = embedding_dict[key]

    # We store the subset of glove, so no need to load the whole glove dict
    with open(GLOVE_SUBSET, 'wb') as fp:
        pickle.dump(subset, fp)


def predict(tokenizer):
    """ Predict the validation and test set using trained model."""
    # Restore the model
    model = load_model()
    load_weight(model)

    # Encode predicting set
    vali_text, _ = get_text_label(TUNE_TEXT)
    test_text, _ = get_text_label(TEST_TEXT)

    vali_sequence = tokenizer.texts_to_sequences(vali_text)
    vali_data = pad_sequences(vali_sequence, maxlen=MAX_LENGTH)

    test_sequence = tokenizer.texts_to_sequences(test_text)
    test_data = pad_sequences(test_sequence, maxlen=MAX_LENGTH)

    # Predicting
    vali_prediction = model.predict(vali_data, verbose=1)
    test_prediction = model.predict(test_data, verbose=1)

    return vali_prediction, test_prediction


def write_prediction(vali_prediction, test_prediction, prediction_csv):
    """ Convert the softmax activated output layer into an integer indicating
        the classification of that prediction. Then write the results into
        a csv file.
    """
    # Use max value as the classification
    classes = []
    for out in test_prediction:
        classes.append(np.argmax(out) + 1)

    for out in vali_prediction:
        classes.append(np.argmax(out) + 1)

    # Write to the output file
    with open(prediction_csv, 'w') as output:
        output.write('"Id","Prediction"\n')
        for i in range(len(classes)):
            output.write("{},{}\n".format(i + 1, classes[i]))


def main():
    # Prepare for the training
    data = MINI_TEXT if DEBUG else TRAIN_TEXT
    train_text, label = get_text_label(data)

    # Add up all text
    predict_vali_text, _ = get_text_label(TUNE_TEXT)
    predict_test_text, _ = get_text_label(TEST_TEXT)
    all_text = train_text + predict_vali_text + predict_test_text
    print("\nSuccessfully load all data...\n")

    # Make a overall vocabulary bank, and get the encoded data for training
    x_train, y_train, x_tune, y_tune, tokenizer = tensorize_words(train_text,
                                                                  all_text,
                                                                  label)
    word_index = tokenizer.word_index

    # Embedding the overall vocabulary
    embedded = embedding_words(word_index)
    print("Successfully make embedding-word dictionary...\n")

    # Start training
    if not PREDICT:
        train(x_train, y_train, x_tune, y_tune, embedded)

    write_prediction(*predict(tokenizer), PREDICTION_CSV)
    print("\nSuccessfully write predictions\n")


if __name__ == '__main__':
    main()
